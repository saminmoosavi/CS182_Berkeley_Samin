{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imitation Learning with Neural Network Policies\n",
    "In this notebook, you will implement the supervised losses for behavior cloning and use it to train policies for locomotion tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Cython<3\n",
      "  Using cached Cython-0.29.37-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (3.1 kB)\n",
      "Using cached Cython-0.29.37-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
      "Installing collected packages: Cython\n",
      "  Attempting uninstall: Cython\n",
      "    Found existing installation: Cython 3.1.0\n",
      "    Uninstalling Cython-3.1.0:\n",
      "      Successfully uninstalled Cython-3.1.0\n",
      "Successfully installed Cython-0.29.37\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# mkdir /.mujoco\n",
    "# Download https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz and extract mujoco-py210 into /.mujoco\n",
    "# Download activation key https://www.roboti.us/file/mjkey.txt and copy it in /.mujoco\n",
    "# Add following to .bashrc\n",
    "#export LD_LIBRARY_PATH=~/.mujoco/mujoco210/bin/\n",
    "#export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/nvidia\n",
    "\n",
    "# sudo apt install libglew-dev\n",
    "#sudo apt-get install patchelf\n",
    "#pip install \"Cython<3\"\n",
    "#mujoco-py 2.1.2.14\n",
    "#pip install protobuf==3.20.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/protobuf/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting protobuf==3.20.1\n",
      "  Using cached protobuf-3.20.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (698 bytes)\n",
      "Using cached protobuf-3.20.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.29.4\n",
      "    Uninstalling protobuf-5.29.4:\n",
      "      Successfully uninstalled protobuf-5.29.4\n",
      "Successfully installed protobuf-3.20.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install protobuf==3.20.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iea/miniconda3/envs/hw4/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#@title imports\n",
    "# As usual, a bit of setup\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import deeprl.infrastructure.pytorch_util as ptu\n",
    "\n",
    "from deeprl.infrastructure.rl_trainer import RL_Trainer\n",
    "from deeprl.infrastructure.trainers import BC_Trainer\n",
    "from deeprl.agents.bc_agent import BCAgent\n",
    "from deeprl.policies.loaded_gaussian_policy import LoadedGaussianPolicy\n",
    "from deeprl.policies.MLP_policy import MLPPolicySL\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "def remove_folder(path):\n",
    "    # check if folder exists\n",
    "    if os.path.exists(path): \n",
    "        print(\"Clearing old results at {}\".format(path))\n",
    "        # remove if exists\n",
    "        shutil.rmtree(path)\n",
    "    else:\n",
    "        print(\"Folder {} does not exist yet. No old results to delete\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_base_args_dict = dict(\n",
    "    expert_policy_file = 'deeprl/policies/experts/Hopper.pkl', #@param\n",
    "    expert_data = 'deeprl/expert_data/expert_data_Hopper-v2.pkl', #@param\n",
    "    env_name = 'Hopper-v2', #@param ['Ant-v2', 'Humanoid-v2', 'Walker2d-v2', 'HalfCheetah-v2', 'Hopper-v2']\n",
    "    exp_name = 'test_bc', #@param\n",
    "    do_dagger = True, #@param {type: \"boolean\"}\n",
    "    ep_len = 1000, #@param {type: \"integer\"}\n",
    "    save_params = False, #@param {type: \"boolean\"}\n",
    "\n",
    "    # Training\n",
    "    num_agent_train_steps_per_iter = 1000, #@param {type: \"integer\"})\n",
    "    n_iter = 10, #@param {type: \"integer\"})\n",
    "\n",
    "    # batches & buffers\n",
    "    batch_size = 10000, #@param {type: \"integer\"})\n",
    "    eval_batch_size = 1000, #@param {type: \"integer\"}\n",
    "    train_batch_size = 100, #@param {type: \"integer\"}\n",
    "    max_replay_buffer_size = 1000000, #@param {type: \"integer\"}\n",
    "\n",
    "    #@markdown network\n",
    "    n_layers = 2, #@param {type: \"integer\"}\n",
    "    size = 64, #@param {type: \"integer\"}\n",
    "    learning_rate = 5e-3, #@param {type: \"number\"}\n",
    "\n",
    "    #@markdown logging\n",
    "    video_log_freq = -1, #@param {type: \"integer\"}\n",
    "    scalar_log_freq = 1, #@param {type: \"integer\"}\n",
    "\n",
    "    #@markdown gpu & run-time settings\n",
    "    no_gpu = False, #@param {type: \"boolean\"}\n",
    "    which_gpu = 0, #@param {type: \"integer\"}\n",
    "    seed = 2, #@param {type: \"integer\"}\n",
    "    logdir = 'test',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infrastructure\n",
    "**Policies**: We have provided implementations of simple neural network policies for your convenience. For discrete environments, the neural network takes in the current state and outputs the logits of the policy's action distribution at this state. The policy then outputs a categorical distribution using those logits. In environments with continuous action spaces, the network will output the mean of a diagonal Gaussian distribution, as well as having a separate single parameter for the log standard deviations of the Gaussian. \n",
    "\n",
    "Calling forward on the policy will output a torch distribution object, so look at the documentation at https://pytorch.org/docs/stable/distributions.html.\n",
    "Look at <code>policies/MLP_policy</code> to make sure you understand the implementation.\n",
    "\n",
    "**RL Training Loop**: The reinforcement learning training loop, which alternates between gathering samples from the environment and updating the policy (and other learned functions) can be found in <code>infrastructure/rl_trainer.py</code>. While you won't need to understand this for the basic behavior cloning part (as you only use a fixed set of expert data), you should read through and understand the run_training_loop function before starting the Dagger implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Behavior Cloning\n",
    "The first part of the assignment will be a familiar exercise in supervised learning. Given a dataset of expert trajectories, we will simply train our policy to imitate the expert via maximum likelihood. Fill out the update method in the MLPPolicySL class in <code>policies/MLP_policy.py</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight before update [[-0.00432252  0.30971584 -0.47518533]\n",
      " [-0.4248946  -0.22236897  0.15482073]]\n",
      "13.142094\n",
      "Loss Error 0.6666666383109463 should be on the order of 1e-6 or lower\n",
      "Weight after update [[ 0.03953311 -0.1517014  -1.536507  ]\n",
      " [-0.21503    -1.4284428  -0.84785604]]\n",
      "Change in weights [[ 0.04385563 -0.46141726 -1.0613217 ]\n",
      " [ 0.2098646  -1.2060739  -1.0026767 ]]\n",
      "Weight Update Error 1.894128460258801e-06 should be on the order of 1e-6 or lower\n"
     ]
    }
   ],
   "source": [
    "### Basic test for correctness of loss and gradients\n",
    "torch.manual_seed(0)\n",
    "ac_dim = 2\n",
    "ob_dim = 3\n",
    "batch_size = 5\n",
    "\n",
    "policy = MLPPolicySL(\n",
    "            ac_dim=ac_dim,\n",
    "            ob_dim=ob_dim,\n",
    "            n_layers=1,\n",
    "            size=2,\n",
    "            learning_rate=0.25)\n",
    "\n",
    "np.random.seed(0)\n",
    "obs = np.random.normal(size=(batch_size, ob_dim))\n",
    "acts = np.random.normal(size=(batch_size, ac_dim))\n",
    "\n",
    "first_weight_before = np.array(ptu.to_numpy(next(policy.mean_net.parameters())))\n",
    "print(\"Weight before update\", first_weight_before)\n",
    "\n",
    "for i in range(5):\n",
    "    loss = policy.update(obs, acts)['Training Loss']\n",
    "\n",
    "print(loss)\n",
    "expected_loss = 2.628419\n",
    "loss_error = rel_error(loss, expected_loss)\n",
    "print(\"Loss Error\", loss_error, \"should be on the order of 1e-6 or lower\")\n",
    "\n",
    "first_weight_after = ptu.to_numpy(next(policy.mean_net.parameters()))\n",
    "print('Weight after update', first_weight_after)\n",
    "\n",
    "weight_change = first_weight_after - first_weight_before\n",
    "print(\"Change in weights\", weight_change)\n",
    "\n",
    "expected_change = np.array([[ 0.04385546, -0.4614172,  -1.0613215 ],\n",
    "                            [ 0.20986436, -1.2060736,  -1.0026767 ]])\n",
    "updated_weight_error = rel_error(weight_change, expected_change)\n",
    "print(\"Weight Update Error\", updated_weight_error, \"should be on the order of 1e-6 or lower\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having implemented our behavior cloning loss, we can now start training some policies to imitate the expert policies provided. \n",
    "\n",
    "Run the following cell to train policies with simple behavior cloning on the HalfCheetah environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing old results at logs/behavior_cloning/HalfCheetah\n",
      "Running behavior cloning experiment with seed 0\n",
      "########################\n",
      "logging outputs to  logs/behavior_cloning/HalfCheetah/seed0\n",
      "########################\n",
      "Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n",
      "HalfCheetah-v2\n",
      "Loading expert policy from... deeprl/policies/experts/HalfCheetah.pkl\n",
      "obs (1, 17) (1, 17)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3907.33447265625\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 3907.33447265625\n",
      "Eval_MinReturn : 3907.33447265625\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4205.7783203125\n",
      "Train_StdReturn : 83.038818359375\n",
      "Train_MaxReturn : 4288.81689453125\n",
      "Train_MinReturn : 4122.7392578125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 1.9295806884765625\n",
      "Training Loss : -789.9697265625\n",
      "Initial_DataCollection_AverageReturn : 4205.7783203125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Running behavior cloning experiment with seed 1\n",
      "########################\n",
      "logging outputs to  logs/behavior_cloning/HalfCheetah/seed1\n",
      "########################\n",
      "Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n",
      "HalfCheetah-v2\n",
      "Loading expert policy from... deeprl/policies/experts/HalfCheetah.pkl\n",
      "obs (1, 17) (1, 17)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3962.109130859375\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 3962.109130859375\n",
      "Eval_MinReturn : 3962.109130859375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4205.7783203125\n",
      "Train_StdReturn : 83.038818359375\n",
      "Train_MaxReturn : 4288.81689453125\n",
      "Train_MinReturn : 4122.7392578125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 2.095921754837036\n",
      "Training Loss : -836.0007934570312\n",
      "Initial_DataCollection_AverageReturn : 4205.7783203125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Running behavior cloning experiment with seed 2\n",
      "########################\n",
      "logging outputs to  logs/behavior_cloning/HalfCheetah/seed2\n",
      "########################\n",
      "Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n",
      "HalfCheetah-v2\n",
      "Loading expert policy from... deeprl/policies/experts/HalfCheetah.pkl\n",
      "obs (1, 17) (1, 17)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4108.2294921875\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 4108.2294921875\n",
      "Eval_MinReturn : 4108.2294921875\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4205.7783203125\n",
      "Train_StdReturn : 83.038818359375\n",
      "Train_MaxReturn : 4288.81689453125\n",
      "Train_MinReturn : 4122.7392578125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 2.2116668224334717\n",
      "Training Loss : -908.7780151367188\n",
      "Initial_DataCollection_AverageReturn : 4205.7783203125\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bc_args = dict(bc_base_args_dict)\n",
    "\n",
    "env_str = 'HalfCheetah'\n",
    "bc_args['expert_policy_file'] = 'deeprl/policies/experts/{}.pkl'.format(env_str)\n",
    "bc_args['expert_data'] = 'deeprl/expert_data/expert_data_{}-v2.pkl'.format(env_str)\n",
    "bc_args['env_name'] = '{}-v2'.format(env_str)\n",
    "\n",
    "# Delete all previous logs\n",
    "remove_folder('logs/behavior_cloning/{}'.format(env_str))\n",
    "\n",
    "for seed in range(3):\n",
    "    print(\"Running behavior cloning experiment with seed\", seed)\n",
    "    bc_args['seed'] = seed\n",
    "    bc_args['logdir'] = 'logs/behavior_cloning/{}/seed{}'.format(env_str, seed)\n",
    "    bctrainer = BC_Trainer(bc_args)\n",
    "    bctrainer.run_training_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize your results using Tensorboard. You should see that on HalfCheetah, the returns of your learned policies (Eval_AverageReturn) are fairly similar (thought a bit lower) to that of the expert (Initial_DataCollection_Average_Return)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3942800087.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[25], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    fuser 6006/tcp\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "### Visualize behavior cloning results on HalfCheetah\n",
    "#fuser 6006/tcp\n",
    "#kill -9 98329\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/behavior_cloning/HalfCheetah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the following cell to train policies with simple behavior cloning on Hopper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing old results at logs/behavior_cloning/Hopper\n",
      "Running behavior cloning experiment on Hopper with seed 0\n",
      "########################\n",
      "logging outputs to  logs/behavior_cloning/Hopper/seed0\n",
      "########################\n",
      "Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n",
      "Hopper-v2\n",
      "Loading expert policy from... deeprl/policies/experts/Hopper.pkl\n",
      "obs (1, 11) (1, 11)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1150.5394287109375\n",
      "Eval_StdReturn : 559.3449096679688\n",
      "Eval_MaxReturn : 1808.390625\n",
      "Eval_MinReturn : 441.1878356933594\n",
      "Eval_AverageEpLen : 333.3333333333333\n",
      "Train_AverageReturn : 3772.67041015625\n",
      "Train_StdReturn : 1.9483642578125\n",
      "Train_MaxReturn : 3774.61865234375\n",
      "Train_MinReturn : 3770.721923828125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 1.938983678817749\n",
      "Training Loss : -395.9472351074219\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Running behavior cloning experiment on Hopper with seed 1\n",
      "########################\n",
      "logging outputs to  logs/behavior_cloning/Hopper/seed1\n",
      "########################\n",
      "Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n",
      "Hopper-v2\n",
      "Loading expert policy from... deeprl/policies/experts/Hopper.pkl\n",
      "obs (1, 11) (1, 11)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 973.4810791015625\n",
      "Eval_StdReturn : 82.43167114257812\n",
      "Eval_MaxReturn : 1112.966796875\n",
      "Eval_MinReturn : 904.5537719726562\n",
      "Eval_AverageEpLen : 288.0\n",
      "Train_AverageReturn : 3772.67041015625\n",
      "Train_StdReturn : 1.9483642578125\n",
      "Train_MaxReturn : 3774.61865234375\n",
      "Train_MinReturn : 3770.721923828125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 2.0076498985290527\n",
      "Training Loss : -356.2700500488281\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Running behavior cloning experiment on Hopper with seed 2\n",
      "########################\n",
      "logging outputs to  logs/behavior_cloning/Hopper/seed2\n",
      "########################\n",
      "Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n",
      "Hopper-v2\n",
      "Loading expert policy from... deeprl/policies/experts/Hopper.pkl\n",
      "obs (1, 11) (1, 11)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1254.407470703125\n",
      "Eval_StdReturn : 137.13525390625\n",
      "Eval_MaxReturn : 1447.4300537109375\n",
      "Eval_MinReturn : 1141.59228515625\n",
      "Eval_AverageEpLen : 362.0\n",
      "Train_AverageReturn : 3772.67041015625\n",
      "Train_StdReturn : 1.9483642578125\n",
      "Train_MaxReturn : 3774.61865234375\n",
      "Train_MinReturn : 3770.721923828125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 2.0016722679138184\n",
      "Training Loss : -359.5938720703125\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bc_args = dict(bc_base_args_dict)\n",
    "\n",
    "env_str = 'Hopper'\n",
    "bc_args['expert_policy_file'] = 'deeprl/policies/experts/{}.pkl'.format(env_str)\n",
    "bc_args['expert_data'] = 'deeprl/expert_data/expert_data_{}-v2.pkl'.format(env_str)\n",
    "bc_args['env_name'] = '{}-v2'.format(env_str)\n",
    "\n",
    "# Delete all previous logs\n",
    "remove_folder('logs/behavior_cloning/{}'.format(env_str))\n",
    "\n",
    "for seed in range(3):\n",
    "    print(\"Running behavior cloning experiment on Hopper with seed\", seed)\n",
    "    bc_args['seed'] = seed\n",
    "    bc_args['logdir'] = 'logs/behavior_cloning/{}/seed{}'.format(env_str, seed)\n",
    "    bctrainer = BC_Trainer(bc_args)\n",
    "    bctrainer.run_training_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize your results using Tensorboard. You should see that on Hopper, the returns of your learned policies (Eval_AverageReturn) are substantially lower than that of the expert (Initial_DataCollection_Average_Return), due to the distribution shift issues that arise when doing naive behavior cloning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualize behavior cloning results on Hopper\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/behavior_cloning/Hopper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Aggregation\n",
    "As discussed in lecture, behavior cloning can suffer from distribution shift, as a small mismatch between the learned and expert policy can take the learned policy to new states that were unseen during training, on which the learned policy hasn't been trained. In Dagger, we will address this issue iteratively, where we use our expert policy to provide labels for the new states we encounter with our learned policy, and then retrain our policy on these newly labeled states.\n",
    "\n",
    "Implement the <code>do_relabel_with_expert</code> function in <code>infrastructure/rl_trainer.py</code>. The errors in the expert actions should be on the order of 1e-6 or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test do relabel function\n",
    "bc_args = dict(bc_base_args_dict)\n",
    "\n",
    "env_str = 'Hopper'\n",
    "bc_args['expert_policy_file'] = 'deeprl/policies/experts/{}.pkl'.format(env_str)\n",
    "bc_args['expert_data'] = 'deeprl/expert_data/expert_data_{}-v2.pkl'.format(env_str)\n",
    "bc_args['env_name'] = '{}-v2'.format(env_str)\n",
    "bctrainer = BC_Trainer(bc_args)\n",
    "\n",
    "np.random.seed(0)\n",
    "T = 2\n",
    "ob_dim = 11\n",
    "ac_dim = 3\n",
    "\n",
    "paths = []\n",
    "for i in range(3):\n",
    "    obs = np.random.normal(size=(T, ob_dim))\n",
    "    acs = np.random.normal(size=(T, ac_dim))\n",
    "    paths.append(dict(observation=obs,\n",
    "                      action=acs))\n",
    "    \n",
    "rl_trainer = bctrainer.rl_trainer\n",
    "relabeled_paths = rl_trainer.do_relabel_with_expert(bctrainer.loaded_expert_policy, paths)\n",
    "\n",
    "expert_actions = np.array([[[-1.7814021, -0.11137983,  1.763353  ],\n",
    "                            [-2.589222,   -5.463195,    2.4301376 ]],\n",
    "                           [[-2.8287444, -5.298558,   3.0320463],\n",
    "                            [ 3.9611065,  2.626403,  -2.8639293]],\n",
    "                           [[-0.3055225,  -0.9865407,   0.80830705],\n",
    "                            [ 2.8788857,   3.5550566,  -0.92875874]]])\n",
    "\n",
    "for i, (path, relabeled_path) in enumerate(zip(paths, relabeled_paths)):\n",
    "    assert np.all(path['observation'] == relabeled_path['observation'])\n",
    "    print(\"Path {} expert action error\".format(i), rel_error(expert_actions[i], relabeled_path['action']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run Dagger on the Hopper env again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dagger_args = dict(bc_base_args_dict)\n",
    "\n",
    "dagger_args['do_dagger'] = True\n",
    "dagger_args['n_iter'] = 10\n",
    "\n",
    "env_str = 'Hopper'\n",
    "dagger_args['expert_policy_file'] = 'deeprl/policies/experts/{}.pkl'.format(env_str)\n",
    "dagger_args['expert_data'] = 'deeprl/expert_data/expert_data_{}-v2.pkl'.format(env_str)\n",
    "dagger_args['env_name'] = '{}-v2'.format(env_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all previous logs\n",
    "remove_folder('logs/dagger/{}'.format(env_str))\n",
    "\n",
    "for seed in range(3):\n",
    "    print(\"Running Dagger experiment with seed\", seed)\n",
    "    dagger_args['seed'] = seed\n",
    "    dagger_args['logdir'] = 'logs/dagger/{}/seed{}'.format(env_str, seed)\n",
    "    bctrainer = BC_Trainer(dagger_args)\n",
    "    bctrainer.run_training_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the Dagger results on Hopper, we see that Dagger is able to recover the performance of the expert policy after a few iterations of online interaction and expert relabeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualize Dagger results on Hopper\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/dagger/Hopper"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
